Only in step6_data_parallel_bucket/__pycache__: data.cpython-39.pyc
Binary files step6_data_parallel_bucket/__pycache__/data_parallel.cpython-39.pyc and step7_pipeline_parallel_afab/__pycache__/data_parallel.cpython-39.pyc differ
Only in step7_pipeline_parallel_afab/__pycache__: dataloader.cpython-39.pyc
Binary files step6_data_parallel_bucket/__pycache__/model.cpython-39.pyc and step7_pipeline_parallel_afab/__pycache__/model.cpython-39.pyc differ
Only in step7_pipeline_parallel_afab/__pycache__: pipeline_parallel.cpython-39.pyc
Binary files step6_data_parallel_bucket/__pycache__/process_group_manager.cpython-39.pyc and step7_pipeline_parallel_afab/__pycache__/process_group_manager.cpython-39.pyc differ
Binary files step6_data_parallel_bucket/__pycache__/tensor_parallel.cpython-39.pyc and step7_pipeline_parallel_afab/__pycache__/tensor_parallel.cpython-39.pyc differ
Binary files step6_data_parallel_bucket/__pycache__/utils.cpython-39.pyc and step7_pipeline_parallel_afab/__pycache__/utils.cpython-39.pyc differ
Only in step7_pipeline_parallel_afab/: pipeline_parallel.py
diff -ur step6_data_parallel_bucket/train.py step7_pipeline_parallel_afab/train.py
--- step6_data_parallel_bucket/train.py	2024-11-17 14:56:25.000000000 +0000
+++ step7_pipeline_parallel_afab/train.py	2024-11-17 14:57:04.000000000 +0000
@@ -1,5 +1,5 @@
 """
-torchrun --nproc_per_node 4 train.py --dp_size 4 --micro_batch_size 1 --gradient_accumulation_steps 8 --seq_len 128 --max_tokens 40960 --num_proc 16 --run_name dp_bucket --use_wandb
+torchrun --nproc_per_node 4 train.py --pp_size 4 --pp_engine afab --micro_batch_size 4 --gradient_accumulation_steps 8 --seq_len 128 --max_tokens 40960 --num_proc 16 --run_name pp_afab --use_wandb
 """
 import os
 import time
@@ -21,6 +21,7 @@
 from utils import set_all_seed, print, to_readable_format
 
 from tensor_parallel import apply_tensor_parallel
+from pipeline_parallel import PipelineParallel, train_step_pipeline_afab
 from data_parallel import DataParallelBucket
 
 def all_reduce_loss_across_dp_ranks(loss, device):
@@ -143,6 +144,9 @@
     if pgm.process_group_manager.tp_world_size > 1:
         model = apply_tensor_parallel(model)
 
+    if pgm.process_group_manager.pp_world_size > 1:
+        model = PipelineParallel(model, model_config)
+
     # Need to move the model to the device before wrapping it with DataParallel.
     # Otherwise, the hook will get attached to the CPU model and not the GPU model.
     model.to(dtype).to(device)
@@ -176,6 +180,8 @@
         print("Tokens per step:", to_readable_format(tokens_per_step), is_print_rank=is_wandb_rank)
 
     trained_token, step = 0, 0
+    
+    tensor_shapes = (dataloader.micro_batch_size, dataloader.seq_len, model_config.hidden_size)
 
     dist.barrier()
 
@@ -185,7 +191,13 @@
         step_start_time = time.time()
         optimizer.zero_grad()
 
-        loss = train_step(model, dataloader, device)
+        if pgm.process_group_manager.pp_world_size > 1:
+            if args.pp_engine == "afab":
+                loss = train_step_pipeline_afab(model, dataloader, tensor_shapes, device, dtype)
+            else:
+                raise ValueError(f"Invalid pipeline parallel engine: {args.pp_engine}")
+        else:    
+            loss = train_step(model, dataloader, device)
 
         loss = all_reduce_loss_across_dp_ranks(loss, device)
 
